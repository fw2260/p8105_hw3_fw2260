---
title: "Homework 3"
author: "Lily Wang"
date: "10/4/2020"
output: github_document
---

This is my solution to Homework 3.

```{r setup}
library(tidyverse)
library(hexbin)
library(patchwork)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

Load and clean dataset:

```{r import_instacart}
data("instacart")
```

The `instacart` dataset contains information on online grocery orders from users of Instacart, an online grocery service that allows you to shop online from local stores. Each observation is one item ordered by a user. Specifically, there are variables of the IDs of orders, products, and users, when the order was placed, how many days passed since the last order, the name of the product ordered, and what aisle and department it was from. The dataset has `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Counting the number of aisles:

```{r count_aisles}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are `r nrow(distinct(instacart, aisle))` distinct aisles. The aisles with the most products are fresh vegetables, fresh fruits, packaged vegetables and fruits, and yogurt.

Making a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered:

```{r instacart_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle name",
    y = "Number of items ordered (>10,000)",
    caption = "Data from the instacart dataset"
  )
```

Making a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Number of times each item is ordered is included:

```{r rank_items}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(
    rank = min_rank(desc(n))
  ) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable(digits = 0)
```

Making a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week:

```{r apple_icecream}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(digits = 0)
```


## Problem 2

Importing and wrangling the `accel_data` dataset and adding a weekend vs weekday variable:

```{r clean_accel}
accel_df <- read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  pivot_longer(
    starts_with("activity_"),
    names_to = "activity",
    names_prefix = "activity_",
    values_to = "counts"
  ) %>% 
  mutate(
    activity = as.numeric(activity),
    day = factor(day),
    day = fct_relevel(day, c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")),
    day_type = if_else(day == "Saturday" | day == "Sunday", "weekend", "weekday")) %>% 
  relocate(week, day_id, day, day_type, everything()) %>% 
  arrange(week, day)
```

The current dataset contains accelerometer data of a 63-year-old male who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The accelerometer measures activity counts for each minute of a 24-hour day starting at midnight. Variables include week number, day of the week, whether the day is a weekday or weekend, activity number (corresponding to each of the 1440 minutes of the day) and activity count as measured by the accelerometer. The dataset contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns.

Aggregate across minutes to create a total activity variable for each day:

```{r aggregate_activity}
accel_df %>% 
  group_by(week, day) %>% 
  summarize(
    total_activity = sum(counts)) %>% 
  knitr::kable(digits = 0)
```

Weekends of Week 4 and 5 seem to have lower activity. Let's plot these:

```{r plot_counts}
accel_df %>% 
  ggplot(aes(x = activity, y = counts, color = day)) +
  geom_line() +
  theme(legend.position = "bottom") +
  labs(
    title = "Activity level by day",
    x = "Minute of the day",
    y = "Activity level",
    color = "Day",
    caption = "Data from the Instacart dataset"
  )
```

Overall, activity count is lowest in the first 300 minutes of the day every day of the week and highest around minutes 1200 to 1300. 

## Problem 3

```{r import_noaa}
data("ny_noaa")
```

The `ny_noaa` dataset contains weather information from the National Oceanic and Atmospheric Association for all New York State weather stations from January 1, 1981 through December 31, 2010. Specifically, the dataset contains the weather station ID, date of observation, amount of precipitation (in tenths of mm) and snow (in mm), and minimum and maximum temperature (in tenths of degrees C). Each weather station may collect only a subset of these variables, and therefore the resulting dataset contains extensive missing data, which should not be a large issue. The current dataset is `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns. 

Cleaning the dataset:

```{r clean_ny_noaa}
month_df <-
  tibble(
    month = 1:12,
    month_name = month.name
  )

ny_noaa <-   
  janitor::clean_names(ny_noaa) %>% 
  separate(date, c("year", "month", "day"), "-") %>% 
  mutate(month = as.numeric(month)) %>% 
  left_join(., month_df, by = "month") %>% 
  select(!month) %>%
  relocate(id, year, month_name, everything()) %>% 
  rename(month = month_name) %>% 
  mutate(
    tmin = as.numeric(tmin),
    tmax = as.numeric(tmax),
    tmin = tmin / 10,
    tmax = tmax / 10,
    prcp = prcp / 10
  )
```

Date was separated into year, month, and day. Month was changed from numbers to names. Temperature and precipitation were changed from tenths of a unit to a whole unit. One station had recorded -13 mm of snowfall, which must be a mistake on NOAA's part.

Finding out the most commonly observed values for snowfall:

```{r}
ny_noaa %>%
  count(snow) %>% 
  arrange(desc(n))
```

The most commonly observed values for snowfall are 0 mm and `NA`. 0 mm makes sense because in NY, there are many days where it does not snow. `NA` makes sense because not all stations collected snowfall data all the time.

Plotting average max temperature in January and July in each station across years:

```{r jan_july_plot}
ny_noaa %>% 
  filter(month == c("January", "July")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE)) %>% 
  ungroup() %>% 
  ggplot(aes(x = year, y = mean_tmax, group = id, color = id)) +
  geom_path() +
  facet_grid(~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  theme(legend.position = "none") +
  labs(
    title = "Average maximum temperature in January and July",
    x = "Year",
    y = "Average maximum temperature (\u00B0C)",
    caption = "Data from the NY NOAA dataset"
  )
```

Average maximum temperature is higher in July than January overall, which makes sense as it is summer in NY. There seems to be an overall slight upward trend in max temperature in January with some dips and peaks every two or three years. But overall, temperatures in both months stayed pretty consistent. One station in 1988 and 1984 had a colder than average July.

Plotting `tmax` vs `tmin` on one graph and showing the distribution of snowfall values greater than 0 and less than 100 separately by year on a second graph:

```{r temp_snowfall}
tmax_tmin_plot <- 
  ny_noaa %>% 
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  theme(legend.position = "right") +
  labs(
    title = "Maximum temperature vs Minimum temperature",
    x = "Minimum temp (\u00B0C)",
    y = "Maximum temp (\u00B0C)"
  )

snowfall_plot <-
  ny_noaa %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs(
    title = "Snowfall by Year",
    x = "Year",
    y = "Snowfall (mm)",
    caption = "Data from the NY NOAA dataset"
  )

tmax_tmin_plot / snowfall_plot 
```

For the top plot, as minimum temperature increases, so does maximum temperature. There seems to be the highest numbers of observations that had minimum temperature of around 15 degrees Celsius and maximum temperature of around 25 degrees Celsius. For the bottom plot, the median snowfall every year is 25 mm, with Q1 around 13 mm and Q3 around 50 mm. There are a few years, like 1998, 2006, and 2010, that had less snowfall.